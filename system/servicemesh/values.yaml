alerts:
  enabled: true
  prometheus: kubernetes

owner-info:
  support-group: containers
  helm-chart-url: https://github.com/sapcc/helm-charts/tree/master/system/servicemesh

rootCA:
  #cert:
  #key:

linkerd-crds:
  enabled: false

linkerd-control-plane:
  enabled: false

  #identityTrustAnchorsPEM:

  identity:
    issuer:
      scheme: kubernetes.io/tls
      issuanceLifetime: 72h0m0s

  controllerImage: keppel.global.cloud.sap/ccloud/linkerd/controller

  policyController:
    image:
      name: keppel.global.cloud.sap/ccloud/linkerd/policy-controller

  proxyInit:
    image:
      name: keppel.global.cloud.sap/ccloud/linkerd/proxy-init
    iptablesMode: "nft"
    closeWaitTimeoutSecs: 3600
    privileged: true
    runAsRoot: true

  debugContainer:
    image:
      name: keppel.global.cloud.sap/ccloud/linkerd/debug

  # enables webhook in kube-system
  proxyInjector:
    namespaceSelector:
      matchExpressions:
      - key: config.linkerd.io/admission-webhooks
        operator: NotIn
        values:
        - disabled

  # This values.yaml file contains the values needed to enable HA mode.
  # Usage:
  #   helm install -f values-ha.yaml

  # -- Create PodDisruptionBudget resources for each control plane workload
  enablePodDisruptionBudget: true

  # -- Specify a deployment strategy for each control plane workload
  deploymentStrategy:
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 25%

  # -- add PodAntiAffinity to each control plane workload
  enablePodAntiAffinity: true

  # nodeAffinity: 

  # proxy configuration
  proxy:
    image:
      name: keppel.global.cloud.sap/ccloud/linkerd/proxy

    outboundConnectTimeout: 300s
    inboundConnectTimeout: 300s
    outboundDiscoveryCacheUnusedTimeout: 60s
    inboundDiscoveryCacheUnusedTimeout: 120s

    resources:
      cpu:
        request: 100m
      memory:
        limit: 250Mi
        request: 100Mi

  # controller configuration
  controllerReplicas: 5
  controllerResources: &controller_resources
    cpu: &controller_resources_cpu
      limit: ""
      request: 100m
    memory:
      limit: 1500Mi
      request: 500Mi
  destinationResources: *controller_resources

  # identity configuration
  identityResources:
    cpu: *controller_resources_cpu
    memory:
      limit: 250Mi
      request: 50Mi

  # heartbeat configuration
  heartbeatResources: *controller_resources

  # proxy injector configuration
  proxyInjectorResources:
    cpu: *controller_resources_cpu
    memory:
      limit: 500Mi
      request: 250Mi

  webhookFailurePolicy: Ignore

  # service profile validator configuration
  spValidatorResources: *controller_resources

  # for now in ./templates/podmonitor.yaml until upstream labels are fixed
  podMonitor:
    enabled: false
    labels:
      prometheus: kubernetes

linkerd-viz:
  enabled: false

  #clusterDomain:

  defaultRegistry: keppel.global.cloud.sap/ccloud/linkerd

  resources:
    memory:
      request: 50Mi

  tap:
    resources:
      memory:
        request: 200Mi

  tapInjector:
    resources:
      memory:
        request: 50Mi

  dashboard:
    enforcedHostRegexp: ".*"
    resources:
      memory:
        request: 50Mi

  prometheus:
    enabled: true
    image:
      registry: keppel.global.cloud.sap/ccloud-dockerhub-mirror/prom
    resources:
      memory:
        request: 2Gi
        limit: 10Gi
    globalConfig:
      scrape_interval: 20s
    args:
      storage.tsdb.retention.time: 1h
    scrapeConfigs:
    - job_name: 'linkerd-proxy'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_container_name
        - __meta_kubernetes_pod_container_port_name
        - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
        action: keep
        regex: ^{{default .Values.proxyContainerName "linkerd-proxy" .Values.proxyContainerName}};linkerd-admin;{{.Values.linkerdNamespace}}$
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: pod
      # special case k8s' "job" label, to not interfere with prometheus' "job"
      # label
      # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
      # k8s_job=foo
      - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job]
        action: replace
        target_label: k8s_job
      # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
      - action: labeldrop
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
      # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
      # deployment=foo
      - action: labelmap
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
      # drop all labels that we just made copies of in the previous labelmap
      - action: labeldrop
        regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
      # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
      # foo=bar
      - action: labelmap
        regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
      # Copy all pod labels to tmp labels
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
        replacement: __tmp_pod_label_$1
      # Take `linkerd_io_` prefixed labels and copy them without the prefix
      - action: labelmap
        regex: __tmp_pod_label_linkerd_io_(.+)
        replacement:  __tmp_pod_label_$1
      # Drop the `linkerd_io_` originals
      - action: labeldrop
        regex: __tmp_pod_label_linkerd_io_(.+)
      # Copy tmp labels into real labels
      - action: labelmap
        regex: __tmp_pod_label_(.+)
      metric_relabel_configs:
      - action: drop
        source_labels: [le]
        regex: "2.*|3.*|4.*|5.*"
